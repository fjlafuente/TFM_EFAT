{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as requests\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import urllib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pandas import to_datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import requests as requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction: APIs\n",
    "\n",
    "### REE APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_REE_generation(year): # -> REE API Only allows to extract data in a yearly basis\n",
    "\n",
    "    #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "    response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/estructura-generacion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day\")\n",
    "    \n",
    "    #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "    generation = response.json()['included']\n",
    "    generation = pd.DataFrame.from_dict(generation)['attributes']\n",
    "    generation = pd.json_normalize(generation)\n",
    "    generation = generation[['title','values']]\n",
    "    \n",
    "    #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "    \n",
    "    typelist = list(generation['title'])\n",
    "    n= 0\n",
    "    data_consolidated = pd.DataFrame(columns= ['value', 'percentage', 'datetime', 'Type'])\n",
    "    \n",
    "    for value in generation['values']:\n",
    " \n",
    "        data = pd.DataFrame.from_dict(value)\n",
    "        data['Type'] = typelist[n]\n",
    "        n += 1\n",
    "        data_consolidated = pd.concat([data_consolidated, data])\n",
    "        \n",
    "    return data_consolidated\n",
    "\n",
    "\n",
    "def data_REE_demand(year): # -> REE API Only allows to extract data in a yearly basis\n",
    "\n",
    "     #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "    demand = requests.get(f\"https://apidatos.ree.es/es/datos/demanda/evolucion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day\")\n",
    "\n",
    "    #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "    demand = demand.json()['included']\n",
    "    demand = pd.DataFrame.from_dict(demand)['attributes']\n",
    "    demand = pd.json_normalize(demand)\n",
    "    demand = demand['values']\n",
    "\n",
    "    #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "\n",
    "    n= 0\n",
    "    data_consolidated = pd.DataFrame(columns= ['value', 'percentage', 'datetime'])\n",
    "    \n",
    "    for value in demand:\n",
    " \n",
    "        data = pd.DataFrame.from_dict(value)\n",
    "        n += 1\n",
    "        data_consolidated = pd.concat([data_consolidated, data])\n",
    "    return data_consolidated\n",
    "\n",
    "def data_REE_potencia_instalada(year): # -> REE API Only allows to extract data in a yearly range as much, in monthly basis\n",
    "\n",
    "    #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "    response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/potencia-instalada?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day\")\n",
    "    \n",
    "    #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "    pinstalled = response.json()['included']\n",
    "    pinstalled = pd.DataFrame.from_dict(pinstalled)['attributes']\n",
    "    pinstalled = pd.json_normalize(pinstalled)\n",
    "    pinstalled = pinstalled[['title','values']]\n",
    "    \n",
    "    #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "    \n",
    "    typelist = list(pinstalled['title'])\n",
    "    n= 0\n",
    "    data_consolidated = pd.DataFrame(columns= ['value', 'percentage', 'datetime', 'Type'])\n",
    "    \n",
    "    for value in pinstalled['values']:\n",
    " \n",
    "        data = pd.DataFrame.from_dict(value)\n",
    "        data['Type'] = typelist[n]\n",
    "        n += 1\n",
    "        data_consolidated = pd.concat([data_consolidated, data])\n",
    "        \n",
    "    return data_consolidated\n",
    "\n",
    "def data_REE_generation_by_ccaa(year): #-> In order to know generation per CCAA per month\n",
    "\n",
    "    response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/estructura-generacion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day&all_ccaa=allCcaa\")\n",
    "    \n",
    "    #To convert response into a pd.DataFrame\n",
    "\n",
    "    generation_ccaa = response.json()['included']\n",
    "    generation_ccaa = pd.json_normalize(generation_ccaa)\n",
    "\n",
    "    #In order to include afterwards the reference for the CCAA when processing the file (it has dictionaries, lists... the proccess is complicated)\n",
    "    \n",
    "    ccaa_info = generation_ccaa[['geo_id', 'community_name']]\n",
    "    ccaa_info['ccaa'] = range(0,20)\n",
    "\n",
    "    #We access to 'content' key, where the information we want is, and create an empty dataframe to be fullfilled by iterating in the items\n",
    "\n",
    "    content = generation_ccaa['content'].to_dict()\n",
    "    total_data = pd.DataFrame(columns = ['ccaa', 'month', 'type', 'value', 'datetime', 'percentage'])\n",
    "\n",
    "    #Iteration will be based on keylist due to the response structure:\n",
    "\n",
    "    keylist = list(content.keys())\n",
    "    \n",
    "    for item in keylist:\n",
    "        data = content[item]\n",
    "        ccaa = item\n",
    "        for element in data:\n",
    "            data_selected = element['attributes']\n",
    "            data_selected = data_selected['values']\n",
    "            tech = element['type']\n",
    "            n = 0\n",
    "            for month in data_selected:\n",
    "                    n += 1\n",
    "                    df = pd.DataFrame(month, columns = ['value', 'percentage', 'datetime'], index = [n])\n",
    "                    df['type'] = tech\n",
    "                    df['ccaa'] = ccaa\n",
    "                    df['month'] = n\n",
    "                    total_data = pd.concat([total_data, df])\n",
    "\n",
    "    total_data_info = pd.merge(total_data , ccaa_info, on='ccaa', how = 'inner')\n",
    "\n",
    "    #We will already drop the information of 'total cca' as it won't be necessary, we already have it\n",
    "\n",
    "    data_filt = total_data_info.loc[total_data_info['ccaa'] != 19]\n",
    "\n",
    "    return data_filt\n",
    "\n",
    "\n",
    "def generation_by_CCAA_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    generation = data_REE_generation_by_ccaa(year)\n",
    "    generation.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/Generation_by_CCAA/Generation_ccaa_{year}.csv\", index = False)\n",
    "\n",
    "\n",
    "def demand_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    demand = data_REE_demand(year)\n",
    "    demand.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Demand/Demand_{year}.csv\", index = False)\n",
    "\n",
    "def pinstalled_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    pinstalled = data_REE_potencia_instalada(year)\n",
    "    pinstalled.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/PowerInstalled_{year}.csv\", index = False)\n",
    "\n",
    "def generation_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    generation = data_REE_generation(year)\n",
    "    generation.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/Generation_{year}.csv\", index = False)    \n",
    "\n",
    "def aemet_data_api(year): # -> in order to get weather records from AEMET API\n",
    "\n",
    "    #It only allows to extract data in a mothly basis, so we have to create a for loop to solve it:\n",
    "\n",
    "    monthlist = ('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12')\n",
    "\n",
    "    #To see if year is a leap year:\n",
    "\n",
    "    feb = 'feb'\n",
    "\n",
    "    if year % 100 == 0:\n",
    "        if year % 400 == 0:\n",
    "            feb = '29'\n",
    "        else:\n",
    "            feb = '28'\n",
    "    else:\n",
    "        if year  % 4 == 0:\n",
    "            feb = '29'\n",
    "        else:\n",
    "            feb = '28'\n",
    "\n",
    "    monthlastday = {'01': '31', '02':feb, '03': '31', '04':'30', '05': '31', '06':'30', '07': '31', \n",
    "                    '08': '31', '09':'30', '10': '31', '11':'30', '12': '31'}\n",
    "\n",
    "    aemet_consolidated = pd.DataFrame(columns = ['fecha', 'indicativo', 'nombre', 'provincia', 'altitud', 'tmed', 'prec', 'tmin', 'horatmin', 'tmax',\n",
    "                                    'horatmax', 'dir', 'velmedia', 'racha', 'horaracha', 'sol', 'presMax', 'horaPresMax', 'presMin', 'horaPresMin'])\n",
    "\n",
    "\n",
    "    for month in monthlist:\n",
    "\n",
    "        fechaIniStr = f\"{year}-{month}-01T00:00:00UTC\" # str | Fecha Inicial (AAAA-MM-DDTHH:MM:SSUTC)\n",
    "        fechaFinStr = f\"{year}-{month}-{monthlastday[month]}T23:59:59UTC\"  # str | Fecha Final (AAAA-MM-DDTHH:MM:SSUTC)\n",
    "\n",
    "        url = f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{fechaIniStr}/fechafin/{fechaFinStr}/todasestaciones\"\n",
    "\n",
    "        #We need an API key that can be obtained from AEMET easily\n",
    "        \n",
    "        query = {\"api_key\":\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZpZXIuZXNjYWxvbmlsbGFAaG90bWFpbC5jb20iLCJqdGkiOiJlNzgyMjg0Yy05YjI0LTQ5ZDktOWMwMS1kYjRlZjQwNjkxNDIiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY4MTE0MTIyNCwidXNlcklkIjoiZTc4MjI4NGMtOWIyNC00OWQ5LTljMDEtZGI0ZWY0MDY5MTQyIiwicm9sZSI6IiJ9.3flzKWh31FkeRBFex1xc4nIwEaQE1QPXoCpeicIluQU\"}\n",
    "\n",
    "        #We have to process the response from requests a little bit as the response is a url\n",
    "\n",
    "        response = requests.request(\"GET\", url,  params = query)\n",
    "\n",
    "        aemet_data = response.json()['datos']\n",
    "\n",
    "        aemet_data = urllib.request.urlopen(aemet_data)\n",
    "\n",
    "        # UTF-8 decoding, which is the standard, does not work with some characters of the response\n",
    "\n",
    "        aemet_data = json.loads(aemet_data.read().decode('latin-1'))\n",
    "        \n",
    "        aemet_data_df = pd.DataFrame.from_dict(aemet_data)\n",
    "\n",
    "        aemet_consolidated = pd.concat([aemet_consolidated, aemet_data_df])\n",
    "    \n",
    "    return aemet_consolidated\n",
    "\n",
    "\n",
    "\n",
    "def weather_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    weather = aemet_data_api(year)\n",
    "    weather.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Weather/Weather{year}.csv\", index = False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_processing(weather_file):\n",
    "\n",
    "    weather_file_processed = weather_file\n",
    "\n",
    "    #Date to datetime:\n",
    "\n",
    "    weather_file_processed['fecha'] = weather_file_processed['fecha'].apply(to_datetime)\n",
    "\n",
    "    #In order to replace 'Ip' values in 'prec':\n",
    "\n",
    "    weather_file_processed.loc[weather_file_processed['prec'] == 'Ip', 'prec'] = 0\n",
    "    \n",
    "   #Let's transform columns into numeric: \n",
    "\n",
    "    columns_to_transform = ['prec', 'velmedia', 'racha', 'tmed', 'tmin', 'tmax', 'sol']\n",
    "    \n",
    "    for column in columns_to_transform:\n",
    "        weather_file_processed[column] = weather_file_processed[column].str.replace(',', '.')\n",
    "        weather_file_processed[column] = weather_file_processed[column].apply(pd.to_numeric)\n",
    "\n",
    "    return weather_file_processed\n",
    "\n",
    "def constrain_weather (weather_file): \n",
    "\n",
    "    #In order to simplify the data, eliminating the geo info by making the mean per day\n",
    "    \n",
    "    weather_pivot_table = weather_file[['fecha', 'provincia', 'tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']]\n",
    "    weather_pivot_table = weather_file.groupby(['fecha', 'provincia'], as_index=False)[['tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']].mean()\n",
    "    weather_pivot_table = weather_pivot_table.groupby('fecha', as_index = False)[['tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']].mean()\n",
    "\n",
    "    return weather_pivot_table\n",
    "\n",
    "def embalses_elect_year (year):\n",
    "\n",
    "    #Firstly, we read the 'embalses' data and select only the year we want and to process 'Embalse Nombre' in order to match it with 'presas' file.\n",
    "    #We only want the dams with the electricty flag marked\n",
    "    \n",
    "    embalses_elect = pd.read_excel('./Data/Hidro/T_Embalses_2014_2023.xlsx')\n",
    "    embalses_elect = embalses_elect.loc[embalses_elect['ELECTRICO_FLAG'] == 1]\n",
    "    embalses_elect['year'] = pd.DatetimeIndex(embalses_elect['FECHA']).year\n",
    "\n",
    "    #In order to transform water KPIs into numeric\n",
    "    str(embalses_elect['AGUA_TOTAL'])\n",
    "    embalses_elect['AGUA_TOTAL'] = embalses_elect['AGUA_TOTAL'].str.replace(',' , '.')\n",
    "    embalses_elect['AGUA_TOTAL'] = embalses_elect['AGUA_TOTAL'].apply(pd.to_numeric)\n",
    "    str(embalses_elect['AGUA_ACTUAL'])\n",
    "    embalses_elect['AGUA_ACTUAL'] = embalses_elect['AGUA_ACTUAL'].str.replace(',' , '.')\n",
    "    embalses_elect['AGUA_ACTUAL'] = embalses_elect['AGUA_ACTUAL'].apply(pd.to_numeric)\n",
    "\n",
    "    #In order to have the information of the year that we want and to get the name of the dam in the way to match it with 'presas' file\n",
    "\n",
    "    embalses_elect = embalses_elect.query('year == @year')\n",
    "    embalses_elect['EMBALSE_NOMBRE'] = embalses_elect['EMBALSE_NOMBRE'].apply(str.upper).apply(unidecode)\n",
    "\n",
    "    #Now, we have to preprocess 'presas' data\n",
    "\n",
    "    presas = pd.read_csv('./Data/Hidro/Presas.csv', encoding = 'latin-1', sep = ';')\n",
    "    presas.rename({'Presa':'EMBALSE_NOMBRE'}, axis = 1, inplace = True)\n",
    "    presas['EMBALSE_NOMBRE'] = presas['EMBALSE_NOMBRE'].apply(unidecode)\n",
    "\n",
    "    #At last, the joint. It will be some dams not matched but are not relevant for our study:\n",
    "\n",
    "    embalses_capacity = embalses_elect.merge(presas, on = 'EMBALSE_NOMBRE', how= 'inner')\n",
    "    \n",
    "    return embalses_capacity"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model (X,y):\n",
    "\n",
    "    #Estimator would be LR from sklearn\n",
    "\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    # Get the number of the features we are including to reshape accordingly afterwards\n",
    "    shape = X.shape[1]\n",
    "\n",
    "    #Reshape in order to have data adjusted\n",
    "\n",
    "    X = X.reshape(-1,shape)\n",
    "    y = y.reshape(-1,1)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    #Fit in order to train the data\n",
    "    reg.fit(X,y)\n",
    "\n",
    "    #We split now the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15)\n",
    "\n",
    "    #Metrics of the model we want to know\n",
    "\n",
    "    predictions = reg.predict(X).round(2)\n",
    "    MAE = np.round(mean_absolute_error(y_test, reg.predict(X_test)),2)\n",
    "    r2 = np.round(r2_score(y_test, reg.predict(X_test)),3)\n",
    "    MSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = True),2)\n",
    "    RMSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = False),2)\n",
    "    cv = cross_val_score(reg, X, y, cv = 5)\n",
    "    cv_mean = np.round(cv.mean(), 3)\n",
    "    cv_std = np.round(cv.std(), 3)\n",
    "\n",
    "\n",
    "    #Outputs\n",
    "    \n",
    "    print(f'Model intercept is {reg.intercept_.round(2)}')\n",
    "    print(f'Model coefficient is {reg.coef_.round(2)}')\n",
    "    print(f'Model MAE is {MAE}')\n",
    "    print(f'Model MSE is {MSE}')\n",
    "    print(f'Model r2 is {r2}')\n",
    "    print(f'Model RMSE is {RMSE}')\n",
    "    print(f'CV mean is {cv_mean} and std is {cv_std}')\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model (X,y):\n",
    "\n",
    "    #Estimator would be KNN from sklearn\n",
    "\n",
    "    reg = KNeighborsRegressor()\n",
    "\n",
    "    # Get the number of the features we are including to reshape accordingly afterwards\n",
    "    shape = X.shape[1]\n",
    "\n",
    "    #Reshape in order to have data adjusted\n",
    "\n",
    "    X = X.reshape(-1,shape)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    #Fit in order to train the data\n",
    "    reg.fit(X,y)\n",
    "\n",
    "    #We split now the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15)\n",
    "\n",
    "    #Metrics of the model we want to know\n",
    "\n",
    "    predictions = reg.predict(X).round(2)\n",
    "    MAE = np.round(mean_absolute_error(y_test, reg.predict(X_test)),2)\n",
    "    r2 = np.round(r2_score(y_test, reg.predict(X_test)),3)\n",
    "    MSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = True),2)\n",
    "    RMSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = False),2)\n",
    "    cv = cross_val_score(reg, X, y, cv = 5)\n",
    "    cv_mean = np.round(cv.mean(), 3)\n",
    "    cv_std = np.round(cv.std(), 3)\n",
    "\n",
    "\n",
    "    #Outputs\n",
    "    \n",
    " \n",
    "    print(f'Model MAE is {MAE}')\n",
    "    print(f'Model MSE is {MSE}')\n",
    "    print(f'Model r2 is {r2}')\n",
    "    print(f'Model RMSE is {RMSE}')\n",
    "    print(f'CV mean is {cv_mean} and std is {cv_std}')\n",
    "\n",
    "    return predictions"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
