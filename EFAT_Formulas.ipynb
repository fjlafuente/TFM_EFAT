{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests as requests\n",
    "import json\n",
    "from requests.auth import HTTPBasicAuth\n",
    "import urllib\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from pandas import to_datetime\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from unidecode import unidecode\n",
    "import pandas as pd\n",
    "import requests as requests\n",
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "import statistics\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Extraction: APIs\n",
    "\n",
    "### REE APIs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_REE_generation(year): # -> REE API Only allows to extract data in a yearly basis\n",
    "\n",
    "    #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "    response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/estructura-generacion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day\")\n",
    "    \n",
    "    #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "    generation = response.json()['included']\n",
    "    generation = pd.DataFrame.from_dict(generation)['attributes']\n",
    "    generation = pd.json_normalize(generation)\n",
    "    generation = generation[['title','values']]\n",
    "    \n",
    "    #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "    \n",
    "    typelist = list(generation['title'])\n",
    "    n= 0\n",
    "    data_consolidated = pd.DataFrame(columns= ['value', 'percentage', 'datetime', 'Type'])\n",
    "    \n",
    "    for value in generation['values']:\n",
    " \n",
    "        data = pd.DataFrame.from_dict(value)\n",
    "        data['Type'] = typelist[n]\n",
    "        n += 1\n",
    "        data_consolidated = pd.concat([data_consolidated, data])\n",
    "        \n",
    "    return data_consolidated\n",
    "\n",
    "\n",
    "def data_REE_demand(year): # -> REE API Only allows to extract data in a yearly basis\n",
    "\n",
    "     #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "    demand = requests.get(f\"https://apidatos.ree.es/es/datos/demanda/evolucion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day\")\n",
    "\n",
    "    #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "    demand = demand.json()['included']\n",
    "    demand = pd.DataFrame.from_dict(demand)['attributes']\n",
    "    demand = pd.json_normalize(demand)\n",
    "    demand = demand['values']\n",
    "\n",
    "    #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "\n",
    "    n= 0\n",
    "    data_consolidated = pd.DataFrame(columns= ['value', 'percentage', 'datetime'])\n",
    "    \n",
    "    for value in demand:\n",
    " \n",
    "        data = pd.DataFrame.from_dict(value)\n",
    "        n += 1\n",
    "        data_consolidated = pd.concat([data_consolidated, data])\n",
    "    return data_consolidated\n",
    "\n",
    "def data_REE_potencia_instalada(yearini, yearend): # -> REE API Only allows to extract data in a yearly range as much, in monthly basis\n",
    "\n",
    "\n",
    "    #First we get the response from REE (It only allow us to see one year each time). We will create an empty list that will be appended\n",
    "\n",
    "    years = range(yearini,yearend + 1)\n",
    "    all_data = []\n",
    "\n",
    "    for year in years:\n",
    "\n",
    "        #First we get the response from REE (It only allow us to see one year each time)\n",
    "\n",
    "        response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/potencia-instalada?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day&all_ccaa=allCcaa\")\n",
    "        \n",
    "        #Data comes in a json with dictionaries inside. To access to the data we have to proccess it a little bit with json and dictionary methods.\n",
    "\n",
    "        pinstalled = response.json()['included']\n",
    "\n",
    "        for ccaa in pinstalled:\n",
    "\n",
    "            cc_aa = ccaa['community_name']\n",
    "            \n",
    "            #We create a for loop in order to access to all data in the dicionary and to get the complete list for all the energetic resources\n",
    "            \n",
    "            for item in ccaa['content']:\n",
    "\n",
    "                techonology = item['type']\n",
    "        \n",
    "                for element in item['attributes']['values']:\n",
    "                    \n",
    "                    value = element['value']\n",
    "                    month = element['datetime']\n",
    "                    all_data.append({\n",
    "                        'comunidad_autonoma': cc_aa,\n",
    "                        'type': techonology,\n",
    "                        'month': month,\n",
    "                        'value': value\n",
    "                    })\n",
    "            time.sleep(3.0)\n",
    "    data_consolidated = pd.DataFrame(all_data)\n",
    "    \n",
    "    return data_consolidated\n",
    "\n",
    "def data_REE_generation_by_ccaa(year): #-> In order to know generation per CCAA per month\n",
    "\n",
    "    response = requests.get(f\"https://apidatos.ree.es/es/datos/generacion/estructura-generacion?start_date={year}-01-01T00:00&end_date={year}-12-31T23:59&time_trunc=day&all_ccaa=allCcaa\")\n",
    "    \n",
    "    #To convert response into a pd.DataFrame\n",
    "\n",
    "    generation_ccaa = response.json()['included']\n",
    "    generation_ccaa = pd.json_normalize(generation_ccaa)\n",
    "\n",
    "    #In order to include afterwards the reference for the CCAA when processing the file (it has dictionaries, lists... the proccess is complicated)\n",
    "    \n",
    "    ccaa_info = generation_ccaa[['geo_id', 'community_name']]\n",
    "    ccaa_info['ccaa'] = range(0,20)\n",
    "\n",
    "    #We access to 'content' key, where the information we want is, and create an empty dataframe to be fullfilled by iterating in the items\n",
    "\n",
    "    content = generation_ccaa['content'].to_dict()\n",
    "    total_data = pd.DataFrame(columns = ['ccaa', 'month', 'type', 'value', 'datetime', 'percentage'])\n",
    "\n",
    "    #Iteration will be based on keylist due to the response structure:\n",
    "\n",
    "    keylist = list(content.keys())\n",
    "    \n",
    "    for item in keylist:\n",
    "        data = content[item]\n",
    "        ccaa = item\n",
    "        for element in data:\n",
    "            data_selected = element['attributes']\n",
    "            data_selected = data_selected['values']\n",
    "            tech = element['type']\n",
    "            n = 0\n",
    "            for month in data_selected:\n",
    "                    n += 1\n",
    "                    df = pd.DataFrame(month, columns = ['value', 'percentage', 'datetime'], index = [n])\n",
    "                    df['type'] = tech\n",
    "                    df['ccaa'] = ccaa\n",
    "                    df['month'] = n\n",
    "                    total_data = pd.concat([total_data, df])\n",
    "\n",
    "    total_data_info = pd.merge(total_data , ccaa_info, on='ccaa', how = 'inner')\n",
    "\n",
    "    #We will already drop the information of 'total cca' as it won't be necessary, we already have it\n",
    "\n",
    "    data_filt = total_data_info.loc[total_data_info['ccaa'] != 19]\n",
    "\n",
    "    return data_filt\n",
    "\n",
    "\n",
    "def generation_by_CCAA_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    generation = data_REE_generation_by_ccaa(year)\n",
    "    generation.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/Generation_by_CCAA/Generation_ccaa_{year}.csv\", index = False)\n",
    "\n",
    "\n",
    "def demand_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    demand = data_REE_demand(year)\n",
    "    demand.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Demand/Demand_{year}.csv\", index = False)\n",
    "\n",
    "def pinstalled_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    pinstalled = data_REE_potencia_instalada(year)\n",
    "    pinstalled.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/PowerInstalled_{year}.csv\", index = False)\n",
    "\n",
    "def generation_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    generation = data_REE_generation(year)\n",
    "    generation.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Generation/Generation_{year}.csv\", index = False)    \n",
    "\n",
    "def aemet_data_api(year): # -> in order to get weather records from AEMET API\n",
    "\n",
    "    #It only allows to extract data in a mothly basis, so we have to create a for loop to solve it:\n",
    "\n",
    "    monthlist = ('01', '02', '03', '04', '05', '06', '07', '08', '09', '10', '11', '12')\n",
    "\n",
    "    #To see if year is a leap year:\n",
    "\n",
    "    feb = 'feb'\n",
    "\n",
    "    if year % 100 == 0:\n",
    "        if year % 400 == 0:\n",
    "            feb = '29'\n",
    "        else:\n",
    "            feb = '28'\n",
    "    else:\n",
    "        if year  % 4 == 0:\n",
    "            feb = '29'\n",
    "        else:\n",
    "            feb = '28'\n",
    "\n",
    "    monthlastday = {'01': '31', '02':feb, '03': '31', '04':'30', '05': '31', '06':'30', '07': '31', \n",
    "                    '08': '31', '09':'30', '10': '31', '11':'30', '12': '31'}\n",
    "\n",
    "    aemet_consolidated = pd.DataFrame(columns = ['fecha', 'indicativo', 'nombre', 'provincia', 'altitud', 'tmed', 'prec', 'tmin', 'horatmin', 'tmax',\n",
    "                                    'horatmax', 'dir', 'velmedia', 'racha', 'horaracha', 'sol', 'presMax', 'horaPresMax', 'presMin', 'horaPresMin'])\n",
    "\n",
    "\n",
    "    for month in monthlist:\n",
    "\n",
    "        fechaIniStr = f\"{year}-{month}-01T00:00:00UTC\" # str | Fecha Inicial (AAAA-MM-DDTHH:MM:SSUTC)\n",
    "        fechaFinStr = f\"{year}-{month}-{monthlastday[month]}T23:59:59UTC\"  # str | Fecha Final (AAAA-MM-DDTHH:MM:SSUTC)\n",
    "\n",
    "        url = f\"https://opendata.aemet.es/opendata/api/valores/climatologicos/diarios/datos/fechaini/{fechaIniStr}/fechafin/{fechaFinStr}/todasestaciones\"\n",
    "\n",
    "        #We need an API key that can be obtained from AEMET easily\n",
    "        \n",
    "        query = {\"api_key\":\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZpZXIuZXNjYWxvbmlsbGFAaG90bWFpbC5jb20iLCJqdGkiOiJlNzgyMjg0Yy05YjI0LTQ5ZDktOWMwMS1kYjRlZjQwNjkxNDIiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY4MTE0MTIyNCwidXNlcklkIjoiZTc4MjI4NGMtOWIyNC00OWQ5LTljMDEtZGI0ZWY0MDY5MTQyIiwicm9sZSI6IiJ9.3flzKWh31FkeRBFex1xc4nIwEaQE1QPXoCpeicIluQU\"}\n",
    "\n",
    "        #We have to process the response from requests a little bit as the response is a url\n",
    "\n",
    "        response = requests.request(\"GET\", url,  params = query)\n",
    "\n",
    "        aemet_data = response.json()['datos']\n",
    "\n",
    "        aemet_data = urllib.request.urlopen(aemet_data)\n",
    "\n",
    "        # UTF-8 decoding, which is the standard, does not work with some characters of the response\n",
    "\n",
    "        aemet_data = json.loads(aemet_data.read().decode('latin-1'))\n",
    "        \n",
    "        aemet_data_df = pd.DataFrame.from_dict(aemet_data)\n",
    "\n",
    "        aemet_consolidated = pd.concat([aemet_consolidated, aemet_data_df])\n",
    "    \n",
    "    return aemet_consolidated\n",
    "\n",
    "\n",
    "\n",
    "def weather_csv_file(year): # -> So we can save all years data in our project's directory\n",
    "    weather = aemet_data_api(year)\n",
    "    weather.to_csv(f\"~/data/TFM_EFAT/TFM_EFAT/Data/Weather/Weather{year}.csv\", index = False)\n",
    "    \n",
    "\n",
    "def download_embalses():\n",
    "\n",
    "    #In order to download and to update pur 'Embalses' info file which will contain the basic data for hidro estimations\n",
    "\n",
    "    #The file is stored in 'Ministerio para la Transición Ecológica y reto demográfico' webpage, and it is stored in zip format\n",
    "\n",
    "    embalses = requests.get('https://www.miteco.gob.es/es/agua/temas/evaluacion-de-los-recursos-hidricos/bd-embalses_tcm30-538779.zip')\n",
    "    with zipfile.ZipFile(io.BytesIO(embalses.content), 'r') as zip_embalses:\n",
    "        zip_embalses.extractall('./Data/Hidro/')\n",
    "\n",
    "    #When extracting it we get a mdb file. To transform it to a csv file:\n",
    "\n",
    "    # MDB file path\n",
    "    mdb_file = './Data/Hidro/BD-Embalses.mdb'\n",
    "\n",
    "    # table name. It has to be adjusted within the years\n",
    "    table_name = \"T_Datos Embalses 1988-2023\"\n",
    "\n",
    "    # output CSV file path\n",
    "    output_csv_file = './Data/Hidro/Embalses.csv'\n",
    "\n",
    "    command = f\"mdb-export {mdb_file} \\\"{table_name}\\\"\"\n",
    "    output = subprocess.run(shlex.split(command), capture_output=True, text=True).stdout\n",
    "\n",
    "    # Write the output to the CSV file\n",
    "    with open(output_csv_file, 'w') as f:\n",
    "        f.write(output)\n",
    "\n",
    "    #Convert the csv file into a dataframe\n",
    "\n",
    "    embalses_data = pd.read_csv('./Data/Hidro/Embalses.csv')\n",
    "\n",
    "    return embalses_data\n",
    "\n",
    "def aemet_municipios(): # -> in order to get masterdata from AEMET API\n",
    "\n",
    "    #AEMET API does not allow to extract predictions for all Municipios all the same time, therefore we firstly have to download the masterdata:\n",
    "\n",
    "    url = f\"https://opendata.aemet.es/opendata/api/maestro/municipios\"\n",
    "\n",
    "    #We need an API key that can be obtained from AEMET easily\n",
    "    \n",
    "    query = {\"api_key\":\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZpZXIuZXNjYWxvbmlsbGFAaG90bWFpbC5jb20iLCJqdGkiOiJlNzgyMjg0Yy05YjI0LTQ5ZDktOWMwMS1kYjRlZjQwNjkxNDIiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY4MTE0MTIyNCwidXNlcklkIjoiZTc4MjI4NGMtOWIyNC00OWQ5LTljMDEtZGI0ZWY0MDY5MTQyIiwicm9sZSI6IiJ9.3flzKWh31FkeRBFex1xc4nIwEaQE1QPXoCpeicIluQU\"}\n",
    "\n",
    "    #We create an empty dataframe with the columns that will be appended\n",
    "\n",
    "    aemet_municipios_total = pd.DataFrame(columns = ['latitud', 'id_old', 'url', 'latitud_dec', 'altitud', 'capital',\n",
    "        'num_hab', 'zona_comarcal', 'destacada', 'nombre', 'longitud_dec', 'id',\n",
    "        'longitud'])\n",
    "    \n",
    "    response = requests.request(\"GET\", url,  params = query)\n",
    "\n",
    "    aemet_municipios = response.json()\n",
    "\n",
    "    #The response is a list of dicts, in which each dict is one municipio:\n",
    "\n",
    "    for item in aemet_municipios:\n",
    "\n",
    "        municipio = pd.DataFrame(item, index= [0])\n",
    "        aemet_municipios_total = pd.concat([aemet_municipios_total, municipio])\n",
    "    aemet_municipios_total.set_index('id', inplace = True)\n",
    "\n",
    "    aemet_municipios_total.to_csv('Data/Weather/Municipios_md.csv')\n",
    "\n",
    "def aemet_municipios_predictions(): # -> in order to get predictions for next seven days from AEMET API\n",
    "\n",
    "    #We are going to extract the predictions of all municipios in Spain\n",
    "\n",
    "    municipios = pd.read_csv('Data/Weather/Municipios_md.csv')\n",
    "\n",
    "    #As we are not going to be able to access to all data due to time cand resources consumption, we ordered by num.inhabitants to get to as many provinces as possible\n",
    "\n",
    "    municipios = municipios.sort_values('num_hab', ascending = False)\n",
    "\n",
    "    municipios_id = list(municipios['id'])\n",
    "\n",
    "    #We create a dataframe that will be appended\n",
    "\n",
    "    df_prediction = pd.DataFrame(columns= ['id_municipio', 'nombre', 'provincia', 'fecha', 'tmax', 'tmin', 'estado_cielo', 'viento', 'racha'])\n",
    "\n",
    "    #Now we create a loop in order to access to all the required information for each municipio:\n",
    "    e = 0\n",
    "\n",
    "    nprovincias = []\n",
    "\n",
    "\n",
    "\n",
    "    for municipio in municipios_id:\n",
    "        e+=1\n",
    "        \n",
    "        if e > 130:\n",
    "            break\n",
    "        else:\n",
    "            #In order to avoid blocking AEMET servers:\n",
    "\n",
    "            time.sleep(0.3)\n",
    "\n",
    "            #We only need municipio ID:\n",
    "\n",
    "            municipio = municipio[2:]\n",
    "\n",
    "            url = f\"https://opendata.aemet.es/opendata/api/prediccion/especifica/municipio/diaria/{municipio}\"\n",
    "\n",
    "            #We need an API key that can be obtained from AEMET easily\n",
    "\n",
    "            query = {\"api_key\":\"eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJqYXZpZXIuZXNjYWxvbmlsbGFAaG90bWFpbC5jb20iLCJqdGkiOiJlNzgyMjg0Yy05YjI0LTQ5ZDktOWMwMS1kYjRlZjQwNjkxNDIiLCJpc3MiOiJBRU1FVCIsImlhdCI6MTY4MTE0MTIyNCwidXNlcklkIjoiZTc4MjI4NGMtOWIyNC00OWQ5LTljMDEtZGI0ZWY0MDY5MTQyIiwicm9sZSI6IiJ9.3flzKWh31FkeRBFex1xc4nIwEaQE1QPXoCpeicIluQU\"}\n",
    "\n",
    "            response = requests.request(\"GET\", url,  params = query, timeout = 500)\n",
    "\n",
    "            \n",
    "            if response.status_code != 200:\n",
    "\n",
    "                #We have another API key if the main one fails. We repeat the same procedure:\n",
    "\n",
    "                time.sleep(0.5)\n",
    "\n",
    "                query = {\"api_key\": 'eyJhbGciOiJIUzI1NiJ9.eyJzdWIiOiJsYWZ1bGFmdWVudGVAZ21haWwuY29tIiwianRpIjoiNGZlY2Y3MzMtYTY3Yy00M2ZmLTgxODMtZTM1N2Q0ODc0YzM5IiwiaXNzIjoiQUVNRVQiLCJpYXQiOjE2ODg1MTM0MjksInVzZXJJZCI6IjRmZWNmNzMzLWE2N2MtNDNmZi04MTgzLWUzNTdkNDg3NGMzOSIsInJvbGUiOiIifQ.WCoq7p6RyV6thyXkkzUmgg27jt2AsJBK3uVTrv2uBcI'}\n",
    "\n",
    "                response = requests.request(\"GET\", url,  params = query, timeout = 500)\n",
    "\n",
    "                predictions = response.json()['datos']\n",
    "\n",
    "                nprovincias.append(municipio[:2])\n",
    "                \n",
    "\n",
    "                predictions = urllib.request.urlopen(predictions)\n",
    "                predictions = json.loads(predictions.read().decode('latin-1'))\n",
    "\n",
    "                #As usual, json file provides the information in many dictionaries, lists etc. and it is not easy to process it:\n",
    "            \n",
    "                for item in predictions:\n",
    "                    item_id = item['id']\n",
    "                    nombre = item['nombre']\n",
    "                    provincia = item['provincia']\n",
    "                    prediccion = item['prediccion']\n",
    "                    prediccion = prediccion['dia']\n",
    "                    for day in prediccion:\n",
    "                        fecha = day['fecha']\n",
    "\n",
    "                        #For get the temperature(max & min):\n",
    "\n",
    "                        temp = day['temperatura']\n",
    "                        tmax = temp['maxima']\n",
    "                        tmin = temp['minima']\n",
    "\n",
    "                        #In order to access to isolation information. We will map the information afterwards\n",
    "\n",
    "                        estado_del_cielo = []\n",
    "                        for data in day['estadoCielo']:\n",
    "\n",
    "                            if data['value'] == '':\n",
    "                                pass\n",
    "                            else:\n",
    "                                estado_id = data['value']\n",
    "                                estado_del_cielo.append(estado_id)\n",
    "\n",
    "                        #Wind information\n",
    "\n",
    "                        velocidad_viento = []\n",
    "                        for data in day['viento']:\n",
    "                            \n",
    "                            velocidad = float(data['velocidad'])\n",
    "                            velocidad_viento.append(velocidad)\n",
    "\n",
    "                        racha_viento = []\n",
    "                        for data in day['rachaMax']:\n",
    "\n",
    "                            if data['value'] == '':\n",
    "                                pass\n",
    "                            else:\n",
    "                                rachamax = float(data['value'])\n",
    "                                racha_viento.append(rachamax)\n",
    "                        \n",
    "                        #Now the dataframe is created:\n",
    "\n",
    "                        estado_cielo = statistics.mode(estado_del_cielo)\n",
    "                        viento = statistics.mean(velocidad_viento)\n",
    "                        if len(racha_viento) == 0: #In order to avoid making the mean with '0' values that are errors:\n",
    "                            racha = None\n",
    "                        else:\n",
    "                            racha = statistics.mean(racha_viento)\n",
    "                        datadict = {'id_municipio': item_id, 'nombre': nombre, 'provincia':provincia, 'fecha': fecha, 'tmax': tmax, 'tmin':tmin, 'estado_cielo' : estado_cielo, 'viento': viento, 'racha' :racha}\n",
    "                        total_prediction = pd.DataFrame(datadict, index = [0])\n",
    "                        df_prediction = pd.concat([df_prediction, total_prediction])\n",
    "                \n",
    "                \n",
    "                    \n",
    "\n",
    "            else:\n",
    "                predictions = response.json()['datos']\n",
    "\n",
    "                nprovincias.append(municipio[:2])\n",
    "                \n",
    "\n",
    "                predictions = urllib.request.urlopen(predictions)\n",
    "                predictions = json.loads(predictions.read().decode('latin-1'))\n",
    "\n",
    "                #As usual, json file provides the information in many dictionaries, lists etc. and it is not easy to process it:\n",
    "            \n",
    "                for item in predictions:\n",
    "                    item_id = item['id']\n",
    "                    nombre = item['nombre']\n",
    "                    provincia = item['provincia']\n",
    "                    prediccion = item['prediccion']\n",
    "                    prediccion = prediccion['dia']\n",
    "                    for day in prediccion:\n",
    "                        fecha = day['fecha']\n",
    "\n",
    "                        #For get the temperature(max & min):\n",
    "\n",
    "                        temp = day['temperatura']\n",
    "                        tmax = temp['maxima']\n",
    "                        tmin = temp['minima']\n",
    "\n",
    "                        #In order to access to isolation information. We will map the information afterwards\n",
    "\n",
    "                        estado_del_cielo = []\n",
    "                        for data in day['estadoCielo']:\n",
    "\n",
    "                            if data['value'] == '':\n",
    "                                pass\n",
    "                            else:\n",
    "                                estado_id = data['value']\n",
    "                                estado_del_cielo.append(estado_id)\n",
    "\n",
    "                        #Wind information\n",
    "\n",
    "                        velocidad_viento = []\n",
    "                        for data in day['viento']:\n",
    "                            \n",
    "                            velocidad = float(data['velocidad'])\n",
    "                            velocidad_viento.append(velocidad)\n",
    "\n",
    "                        racha_viento = []\n",
    "                        for data in day['rachaMax']:\n",
    "\n",
    "                            if data['value'] == '':\n",
    "                                pass\n",
    "                            else:\n",
    "                                rachamax = float(data['value'])\n",
    "                                racha_viento.append(rachamax)\n",
    "                        \n",
    "                        #Now the dataframe is created:\n",
    "\n",
    "                        estado_cielo = statistics.mode(estado_del_cielo)\n",
    "                        viento = statistics.mean(velocidad_viento)\n",
    "                        if len(racha_viento) == 0: #In order to avoid making the mean with '0' values\n",
    "                            racha = None\n",
    "                        else:\n",
    "                            racha = statistics.mean(racha_viento)\n",
    "                        datadict = {'id_municipio': item_id, 'nombre': nombre, 'provincia':provincia, 'fecha': fecha, 'tmax': tmax, 'tmin':tmin, 'estado_cielo' : estado_cielo, 'viento': viento, 'racha' :racha}\n",
    "                        total_prediction = pd.DataFrame(datadict, index = [0])\n",
    "                        df_prediction = pd.concat([df_prediction, total_prediction])\n",
    "       \n",
    "                               \n",
    "        \n",
    "\n",
    "    print(f'{len(set(nprovincias))}, {e}')\n",
    "    return df_prediction"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATA PROCESSING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weather_processing(weather_file):\n",
    "\n",
    "    weather_file_processed = weather_file\n",
    "\n",
    "    #Date to datetime:\n",
    "\n",
    "    weather_file_processed['fecha'] = weather_file_processed['fecha'].apply(to_datetime)\n",
    "\n",
    "    #In order to replace 'Ip' values in 'prec':\n",
    "\n",
    "    weather_file_processed.loc[weather_file_processed['prec'] == 'Ip', 'prec'] = 0\n",
    "    \n",
    "   #Let's transform columns into numeric: \n",
    "\n",
    "    columns_to_transform = ['prec', 'velmedia', 'racha', 'tmed', 'tmin', 'tmax', 'sol']\n",
    "    \n",
    "    for column in columns_to_transform:\n",
    "        weather_file_processed[column] = weather_file_processed[column].str.replace(',', '.')\n",
    "        weather_file_processed[column] = weather_file_processed[column].apply(pd.to_numeric)\n",
    "\n",
    "    return weather_file_processed\n",
    "\n",
    "def constrain_weather (weather_file): \n",
    "\n",
    "    #In order to simplify the data, eliminating the geo info by making the mean per day\n",
    "    \n",
    "    weather_pivot_table = weather_file[['fecha', 'provincia', 'tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']]\n",
    "    weather_pivot_table = weather_file.groupby(['fecha', 'provincia'], as_index=False)[['tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']].mean()\n",
    "    weather_pivot_table = weather_pivot_table.groupby('fecha', as_index = False)[['tmed', 'prec', 'tmin', 'tmax', 'velmedia', 'racha', 'sol']].mean()\n",
    "\n",
    "    return weather_pivot_table\n",
    "\n",
    "def embalses_elect_year (year):\n",
    "\n",
    "    #Firstly, we read the 'embalses' data and select only the year we want and to process 'Embalse Nombre' in order to match it with 'presas' file.\n",
    "    #We only want the dams with the electricty flag marked\n",
    "    \n",
    "    embalses_elect = pd.read_excel('./Data/Hidro/T_Embalses_2014_2023.xlsx')\n",
    "    embalses_elect = embalses_elect.loc[embalses_elect['ELECTRICO_FLAG'] == 1]\n",
    "    embalses_elect['year'] = pd.DatetimeIndex(embalses_elect['FECHA']).year\n",
    "\n",
    "    #In order to transform water KPIs into numeric\n",
    "    str(embalses_elect['AGUA_TOTAL'])\n",
    "    embalses_elect['AGUA_TOTAL'] = embalses_elect['AGUA_TOTAL'].str.replace(',' , '.')\n",
    "    embalses_elect['AGUA_TOTAL'] = embalses_elect['AGUA_TOTAL'].apply(pd.to_numeric)\n",
    "    str(embalses_elect['AGUA_ACTUAL'])\n",
    "    embalses_elect['AGUA_ACTUAL'] = embalses_elect['AGUA_ACTUAL'].str.replace(',' , '.')\n",
    "    embalses_elect['AGUA_ACTUAL'] = embalses_elect['AGUA_ACTUAL'].apply(pd.to_numeric)\n",
    "\n",
    "    #In order to have the information of the year that we want and to get the name of the dam in the way to match it with 'presas' file\n",
    "\n",
    "    embalses_elect = embalses_elect.query('year == @year')\n",
    "    embalses_elect['EMBALSE_NOMBRE'] = embalses_elect['EMBALSE_NOMBRE'].apply(str.upper).apply(unidecode)\n",
    "\n",
    "    #Now, we have to preprocess 'presas' data\n",
    "\n",
    "    presas = pd.read_csv('./Data/Hidro/Presas.csv', encoding = 'latin-1', sep = ';')\n",
    "    presas.rename({'Presa':'EMBALSE_NOMBRE'}, axis = 1, inplace = True)\n",
    "    presas['EMBALSE_NOMBRE'] = presas['EMBALSE_NOMBRE'].apply(unidecode)\n",
    "\n",
    "    #At last, the joint. It will be some dams not matched but are not relevant for our study:\n",
    "\n",
    "    embalses_capacity = embalses_elect.merge(presas, on = 'EMBALSE_NOMBRE', how= 'inner')\n",
    "    \n",
    "    return embalses_capacity\n",
    "\n",
    "def codificar_sol(x):\n",
    "\n",
    "    #In order to align both files: the used for training and the new data with the same category\n",
    "\n",
    "    if x <= 2:\n",
    "        return 0\n",
    "    if 2 < x <= 4:\n",
    "        return 1\n",
    "    if 4 < x <= 6:\n",
    "        return 2\n",
    "    if 6 < x <= 8:\n",
    "        return 3\n",
    "    if 8 < x <= 10:\n",
    "        return 4\n",
    "    if 10 < x:\n",
    "        return 5\n",
    "    \n",
    "    def filter_consolidated_df(consolidated_file):\n",
    "\n",
    "    ccaa_solartermica = consolidated_file[consolidated_file['Solar térmica'] != 0]['comunidad_autonoma'].unique()\n",
    "    ccaa_solarfotovoltaica = consolidated_file[consolidated_file['Solar fotovoltaica'] != 0]['comunidad_autonoma'].unique()\n",
    "    ccaa_hidraulica = consolidated_file[consolidated_file['Eólica'] != 0]['comunidad_autonoma'].unique()\n",
    "    ccaa_eolica = consolidated_file[consolidated_file['Hidráulica'] != 0]['comunidad_autonoma'].unique()\n",
    "\n",
    "    #Based on the information provided by the Pndas Profile and the other notebooks, we can drop alredy some variables that we know are not relevant for each technology:\n",
    "    \n",
    "    df_solartermica = consolidated_file[consolidated_file['comunidad_autonoma'].isin(ccaa_solartermica)]\n",
    "    df_solartermica = df_solartermica[['fecha', 'comunidad_autonoma','tmed','tmax','velmedia','sol','Solar térmica', 'inst_Solar_térmica']] \\\n",
    "        .rename(columns = {'Solar térmica':'generation'})\n",
    "    df_solarfotovoltaica = consolidated_file[consolidated_file['comunidad_autonoma'].isin(ccaa_solarfotovoltaica)]\n",
    "    df_solarfotovoltaica = df_solarfotovoltaica[['fecha', 'comunidad_autonoma','tmed','tmax','velmedia','sol','Solar fotovoltaica', 'inst_Solar fotovoltaica']] \\\n",
    "        .rename(columns = {'Solar fotovoltaica':'generation'})\n",
    "    df_hidraulica = consolidated_file[consolidated_file['comunidad_autonoma'].isin(ccaa_hidraulica)]\n",
    "    df_hidraulica = df_hidraulica[['fecha','comunidad_autonoma','tmed','prec','AGUA_ACTUAL','AGUA_TOTAL', 'Hidráulica', 'inst_Hidráulica']] \\\n",
    "        .rename(columns = {'Hidráulica':'generation'})\n",
    "    df_eolica = consolidated_file[consolidated_file['comunidad_autonoma'].isin(ccaa_eolica)]\n",
    "    df_eolica = df_eolica[['fecha', 'comunidad_autonoma','tmed', 'prec', 'velmedia', 'racha', 'Eólica', 'inst_Eólica']] \\\n",
    "        .rename(columns = {'Eólica':'generation'})\n",
    "    df_demand = consolidated_file[['fecha', 'comunidad_autonoma','tmed', 'sol', 'Weekday', 'prec', 'demand_ccaa']]\n",
    "\n",
    "    return df_solartermica, df_solarfotovoltaica, df_hidraulica, df_eolica, df_demand\n",
    "\n",
    "def create_spain_map(spain_file):\n",
    "    #To access to the geo data we are using geopandas library, which transforms data into readable dataframes:\n",
    "    spain_file = spain_file.rename(columns = {'NAME_1': 'comunidad_autonoma', 'CC_1':'CODAUTO'})\n",
    "    spain_file['comunidad_autonoma'] = (spain_file['comunidad_autonoma'].apply(unidecode, 'utf-8')).str.upper()\n",
    "    spain_file['comunidad_autonoma'] = spain_file['comunidad_autonoma'].replace({'CEUTA Y MELILLA':'MELILLA',\n",
    "                                                                    'COMUNIDAD VALENCIANA': 'COMUNITAT VALENCIANA',\n",
    "                                                                    'ISLAS BALEARES': 'ILLES BALEARS',\n",
    "                                                                    'ISLAS CANARIAS': 'CANARIAS'})\n",
    "    \n",
    "    #We don't need all the information from the file:\n",
    "\n",
    "    spain_file = spain_file[['comunidad_autonoma', 'geometry']]\n",
    "\n",
    "    return spain_file\n",
    "\n",
    "def translate_canarias(spain_file):\n",
    "    \n",
    "    # Apply translate to 'geometry' column based on condition\n",
    "    spain['geometry'] = spain.apply(\n",
    "    lambda row: translate(row['geometry'], xoff=3, yoff=6) if row['comunidad_autonoma'] == 'CANARIAS' else row['geometry'],\n",
    "    axis=1\n",
    "    )\n",
    "\n",
    "    return spain_file"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VISUALIZATION"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODELLING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_model (X,y):\n",
    "\n",
    "    #Estimator would be LR from sklearn\n",
    "\n",
    "    reg = LinearRegression()\n",
    "\n",
    "    # Get the number of the features we are including to reshape accordingly afterwards\n",
    "    shape = X.shape[1]\n",
    "\n",
    "    #Reshape in order to have data adjusted\n",
    "\n",
    "    X = X.reshape(-1,shape)\n",
    "    y = y.reshape(-1,1)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    #Fit in order to train the data\n",
    "    reg.fit(X,y)\n",
    "\n",
    "    #We split now the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15)\n",
    "\n",
    "    #Metrics of the model we want to know\n",
    "\n",
    "    predictions = reg.predict(X).round(2)\n",
    "    MAE = np.round(mean_absolute_error(y_test, reg.predict(X_test)),2)\n",
    "    r2 = np.round(r2_score(y_test, reg.predict(X_test)),3)\n",
    "    MSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = True),2)\n",
    "    RMSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = False),2)\n",
    "    cv = cross_val_score(reg, X, y, cv = 5)\n",
    "    cv_mean = np.round(cv.mean(), 3)\n",
    "    cv_std = np.round(cv.std(), 3)\n",
    "\n",
    "\n",
    "    #Outputs\n",
    "    \n",
    "    print(f'Model intercept is {reg.intercept_.round(2)}')\n",
    "    print(f'Model coefficient is {reg.coef_.round(2)}')\n",
    "    print(f'Model MAE is {MAE}')\n",
    "    print(f'Model MSE is {MSE}')\n",
    "    print(f'Model r2 is {r2}')\n",
    "    print(f'Model RMSE is {RMSE}')\n",
    "    print(f'CV mean is {cv_mean} and std is {cv_std}')\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn_model (X,y):\n",
    "\n",
    "    #Estimator would be KNN from sklearn\n",
    "\n",
    "    reg = KNeighborsRegressor()\n",
    "\n",
    "    # Get the number of the features we are including to reshape accordingly afterwards\n",
    "    shape = X.shape[1]\n",
    "\n",
    "    #Reshape in order to have data adjusted\n",
    "\n",
    "    X = X.reshape(-1,shape)\n",
    "\n",
    "    print(X.shape)\n",
    "    print(y.shape)\n",
    "    \n",
    "    #Fit in order to train the data\n",
    "    reg.fit(X,y)\n",
    "\n",
    "    #We split now the data into train and test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.15)\n",
    "\n",
    "    #Metrics of the model we want to know\n",
    "\n",
    "    predictions = reg.predict(X).round(2)\n",
    "    MAE = np.round(mean_absolute_error(y_test, reg.predict(X_test)),2)\n",
    "    r2 = np.round(r2_score(y_test, reg.predict(X_test)),3)\n",
    "    MSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = True),2)\n",
    "    RMSE = np.round(mean_squared_error(y_test,reg.predict(X_test), squared = False),2)\n",
    "    cv = cross_val_score(reg, X, y, cv = 5)\n",
    "    cv_mean = np.round(cv.mean(), 3)\n",
    "    cv_std = np.round(cv.std(), 3)\n",
    "\n",
    "\n",
    "    #Outputs\n",
    "    \n",
    " \n",
    "    print(f'Model MAE is {MAE}')\n",
    "    print(f'Model MSE is {MSE}')\n",
    "    print(f'Model r2 is {r2}')\n",
    "    print(f'Model RMSE is {RMSE}')\n",
    "    print(f'CV mean is {cv_mean} and std is {cv_std}')\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Read Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drive_read_file(url):\n",
    "    # Generate the Google Drive download URL\n",
    "    download_url = 'https://drive.google.com/uc?id=' + url.split('/')[-2]\n",
    "    \n",
    "    # Download the file using gdown\n",
    "    file_path = gdown.download(download_url, quiet=False)\n",
    "    \n",
    "    # Read the file directly into a pandas DataFrame\n",
    "    df = pd.read_csv(file_path)\n",
    "    \n",
    "    # Delete the downloaded file\n",
    "    os.remove(file_path)\n",
    "    \n",
    "    return df\n",
    "def read_gpd_file(url):\n",
    "    file_id = url.split('/')[-2]\n",
    "    url_total = 'https://drive.google.com/uc?export=download&id=' + file_id\n",
    "    r = requests.get(url_total)\n",
    "    # Create a temporary directory to include the zip that contains the map files\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    # Save the zip file to the temporary directory so we can access to the info. Otherwise gpd_read is not going to work.\n",
    "    with open(f\"{temp_dir}/file.zip\", \"wb\") as f:\n",
    "        f.write(r.content)\n",
    "    # Extract the zip file\n",
    "    with zipfile.ZipFile(f\"{temp_dir}/file.zip\", 'r') as zip_ref:\n",
    "        zip_ref.extractall(temp_dir)\n",
    "    # Find the .shp file as it is the one that really matters for reading and creating the map\n",
    "    shapefile = [f for f in os.listdir(temp_dir) if f.endswith('.shp')][0]\n",
    "    # Read the shapefile\n",
    "    map_file = gpd.read_file(f\"{temp_dir}/{shapefile}\")\n",
    "    # Remove the temporary directory\n",
    "    shutil.rmtree(temp_dir)\n",
    "    return map_file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_map_generation(map_file, dataframe, year):\n",
    "\n",
    "    dataframe = dataframe[dataframe['fecha'].dt.year == year]\n",
    "    dataframe = dataframe.groupby('comunidad_autonoma')['generation'].sum()\n",
    "    spain = pd.merge(map_file, dataframe, on='comunidad_autonoma', how='left').fillna(0)\n",
    "\n",
    "    #Transform generation into GWh:\n",
    "\n",
    "    spain['generation'] = spain['generation']/1000\n",
    "\n",
    "    # Plot:\n",
    "    fig, ax = plt.subplots(figsize=(14, 10))\n",
    "\n",
    "    # Plot the boundaries between comunidades autónomas\n",
    "    spain['geometry'].boundary.plot(ax=ax, linewidth=0.5, color='black')\n",
    "\n",
    "    # Use ax parameter for both plots\n",
    "    spain.plot(column='generation', legend=True, ax=ax, cmap = 'Blues')\n",
    "    \n",
    "    # Adjust the map limits\n",
    "    ax.set_xlim(-16, 5) \n",
    "    ax.set_ylim(33, 45)\n",
    "\n",
    "    plt.title(f'Generation of by CCAA in GWh, {year}')\n",
    "    plt.xlabel('Longitude')\n",
    "    plt.ylabel('Latitude')\n",
    "    plt.legend()\n",
    "    plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
